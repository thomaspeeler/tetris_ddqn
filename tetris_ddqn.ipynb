{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with all of the necessary imports and library settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import pickle\n",
    "import copy\n",
    "import cv2\n",
    "import datetime\n",
    "import os\n",
    "import shutil\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "# this just ensures that the tensors we use can be\n",
    "# saved w/ pickle w/out throwing a warning\n",
    "# (we have to save tensors with pickle instead of\n",
    "# torch.save bc it's much faster)\n",
    "torch.serialization.add_safe_globals(\n",
    "    [\n",
    "        np._core.multiarray._reconstruct,\n",
    "        np.ndarray,\n",
    "        np.dtypes.ObjectDType,\n",
    "        np._core.multiarray.scalar,\n",
    "        np.dtypes.Int32DType,\n",
    "        np.dtypes.Float64DType,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we set up the Tetris environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each of the 7 tetrominoes is representable in three ways:\n",
    "# -an int in [0, 6]\n",
    "# -a 4x4 binary array\n",
    "# -a 4x2 array of coordinates for the 4x4 array\n",
    "# we will be changing between these representations throughout these\n",
    "# functions that handle the game and pieces themselves\n",
    "# but, generally, pieces go int -> coordinates -> binary array\n",
    "\n",
    "# a note on coordinates and rotations:\n",
    "# the specific positions of each piece in the 4x4 array are based on\n",
    "# how the pieces act in Tetris for the NES\n",
    "# that is, each piece has specific coordinates in the 4x4 array such that when\n",
    "# the 4x4 array is stamped on to the board,\n",
    "# each piece is consistent in behavior.\n",
    "# additionally, this setup means that not all rotations are valid;\n",
    "# for example, the O piece cannot be rotated at all, since it would cause the\n",
    "# piece to move and lose its consistency in behavior to the NES game\n",
    "# in the NES game, an invalid rotation input is sanitized to produce a\n",
    "# valid output\n",
    "# but, here, there is no expectation that\n",
    "# the game env will get any invalid input,\n",
    "# since only valid inputs are shown to the network when it decides how to move\n",
    "\n",
    "# this translates int -> 4x2 array of coordinates of the piece\n",
    "int_to_piececoords = [\n",
    "    [(2, i + 1) for i in range(3)] + [(3, 2)],\n",
    "    [(2, i + 1) for i in range(3)] + [(3, 3)],\n",
    "    [(j + 2, i + 1 + j) for j in range(2) for i in range(2)],\n",
    "    [(i + 2, j + 1) for i in range(2) for j in range(2)],\n",
    "    [(j + 2, i + 2 - j) for j in range(2) for i in range(2)],\n",
    "    [(2, i + 1) for i in range(3)] + [(3, 1)],\n",
    "    [(2, i) for i in range(4)],\n",
    "]\n",
    "int_to_piececoords = [np.array(p) for p in int_to_piececoords]\n",
    "\n",
    "# as mentioned, each piece only has so many valid rotations that\n",
    "# do not break consistency\n",
    "# this translates piece int -> list of valid rotations\n",
    "# where 1 indicates a rotation to the right, -1 indicates a left rotation,\n",
    "# and 0 indicates no rotation\n",
    "valid_rotations = [\n",
    "    [-1, 0, 1, 2],\n",
    "    [-1, 0, 1, 2],\n",
    "    [-1, 0],\n",
    "    [0],\n",
    "    [-1, 0],\n",
    "    [-1, 0, 1, 2],\n",
    "    [0, 1],\n",
    "]\n",
    "\n",
    "\n",
    "class piece:\n",
    "    # a class for a single tetromino in a game of Tetris\n",
    "    # initialized with a int, stores the coordinates,\n",
    "    # binary array, and position of the piece\n",
    "\n",
    "    def __init__(self, pieceint):\n",
    "        self.pieceint = pieceint\n",
    "        self.coords = int_to_piececoords[pieceint]\n",
    "        self.piecearray = self.coordtopiecearray(self.coords)\n",
    "        self.rotations = valid_rotations[pieceint]\n",
    "        self.orient = 0\n",
    "        self.pos = {\"x\": 3, \"y\": -2}\n",
    "\n",
    "    # rotate this teromino in place by dir\n",
    "    def rotate_piece(self, dir: int) -> None:\n",
    "        self.coords = self.rotate_coords(self.coords, dir)\n",
    "        self.orient += dir\n",
    "        self.orient %= 4\n",
    "        self.piecearray = self.coordtopiecearray(self.coords)\n",
    "\n",
    "    # helper, does the array computation on the coordinates for rotation\n",
    "    def rotate_coords(self, piececoords: np.ndarray, dir: int) -> np.ndarray:\n",
    "        match dir % 4:\n",
    "            case 1:\n",
    "                piececoords = piececoords[:, ::-1]\n",
    "                piececoords[:, 1] = 4 - piececoords[:, 1]\n",
    "            case 2:\n",
    "                piececoords = 4 - piececoords\n",
    "            case 3:\n",
    "                piececoords = piececoords[:, ::-1]\n",
    "                piececoords[:, 0] = 4 - piececoords[:, 0]\n",
    "            case _:\n",
    "                pass\n",
    "\n",
    "        return piececoords\n",
    "\n",
    "    # helper, does coordinate array -> binary array\n",
    "    def coordtopiecearray(self, coords: np.ndarray) -> np.ndarray:\n",
    "        ret = np.zeros((4, 4), dtype=np.int8)\n",
    "        ret[coords[:, 0], coords[:, 1]] = 1\n",
    "        return ret\n",
    "\n",
    "\n",
    "# here, we set up the randomizers that Tetris can use\n",
    "n_pieces = len(valid_rotations)\n",
    "\n",
    "\n",
    "# superclass for other randomizers\n",
    "class randomizer(ABC):\n",
    "    def __init__(self, seed=None):\n",
    "        self.reset(seed=seed)\n",
    "\n",
    "    @abstractmethod\n",
    "    def reset(self, seed=None):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_new_piece(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "# NES randomizer class\n",
    "# the NES randomizer works as follows:\n",
    "# -roll an 8-sided die\n",
    "# -if the roll is the same as the result of the previous randomization or\n",
    "#  an 8, roll a 7-sided die and return the result\n",
    "# -otherwise, just return the first roll\n",
    "class NESrandom(randomizer):\n",
    "    def __init__(self, seed=None):\n",
    "        super().__init__(seed)\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        self.last_piece = -1\n",
    "        self.gen = np.random.default_rng(seed=seed)\n",
    "\n",
    "    def get_new_piece(self):\n",
    "        roll = self.gen.integers(0, n_pieces, endpoint=True)\n",
    "        if roll in [self.last_piece, n_pieces]:\n",
    "            roll = self.gen.integers(0, n_pieces, endpoint=False)\n",
    "\n",
    "        self.last_piece = roll\n",
    "        return piece(roll)\n",
    "\n",
    "\n",
    "# bag randomizer class, the randomizer used\n",
    "# by most (all?) modern Tetris implementations\n",
    "# the bag randomizer works as follows:\n",
    "# -start with a bag with one of each tetromino in it\n",
    "# -randomly pull from the bag until it is empty, and then create a new bag\n",
    "class Bagrandom(randomizer):\n",
    "    def __init__(self, seed=None):\n",
    "        super().__init__(seed)\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        self.gen = np.random.default_rng(seed=seed)\n",
    "        self.newbag()\n",
    "\n",
    "    def newbag(self):\n",
    "        self.bag = np.arange(n_pieces)\n",
    "        self.gen.shuffle(self.bag)\n",
    "\n",
    "    def get_new_piece(self):\n",
    "        roll = self.bag[0]\n",
    "        self.bag = self.bag[1:]\n",
    "        if self.bag.shape[0] == 0:\n",
    "            self.newbag()\n",
    "        return piece(roll)\n",
    "\n",
    "\n",
    "# class for the actual tetris game implementation\n",
    "# the board is represented by a 20x10 numpy array, so\n",
    "# each \"tile\" in the game is just one spot in the array\n",
    "# (this is, of course, upscaled for rendering)\n",
    "# plays via the tetrisboard.step function\n",
    "# the board used internally has buffers on the edges to prevent\n",
    "# indexing errors when a piece hits the wall\n",
    "class tetris_board:\n",
    "    def __init__(\n",
    "        self, height: int = 20, width: int = 10, seed=None, randomizer: str = \"bag\"\n",
    "    ):\n",
    "        self.buffer = 3\n",
    "        self.line_clears = 0\n",
    "        self.n_pieces = 0\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        match randomizer:\n",
    "            case \"nes\":\n",
    "                self.piecegen = NESrandom(seed=seed)\n",
    "            case _:\n",
    "                self.piecegen = Bagrandom(seed=seed)\n",
    "        self.board = np.ones((2 * self.buffer + height, 2 * self.buffer + width))\n",
    "\n",
    "        self.reset(seed)\n",
    "\n",
    "    # reset the env back to the start of the game\n",
    "    def reset(self, seed=None) -> None:\n",
    "        self.piecegen.reset(seed=seed)\n",
    "        # clearing board\n",
    "        self.board[self.buffer : -self.buffer, self.buffer : -self.buffer] = 0\n",
    "        self.curr_piece = self.piecegen.get_new_piece()\n",
    "        self.next_piece = self.piecegen.get_new_piece()\n",
    "        self.line_clears = 0\n",
    "        self.n_pieces = 0\n",
    "        self.clear_types = {str(i + 1): 0 for i in range(4)}\n",
    "\n",
    "    # \"working board\", just the board without the buffers\n",
    "    def get_wboard(self) -> np.ndarray:\n",
    "        return self.board[self.buffer : -self.buffer, self.buffer : -self.buffer]\n",
    "\n",
    "    # given a piece (including it's position), place it on the board\n",
    "    def stamp_piece(\n",
    "        self, board: np.ndarray, piece: piece, copy: bool = True\n",
    "    ) -> np.ndarray:\n",
    "        if copy:\n",
    "            board = board.copy()\n",
    "        \n",
    "        # we add the binary array of the piece to the board to place it\n",
    "        # on, since the binary array is the piece representation in the game\n",
    "        board[\n",
    "            self.buffer + piece.pos[\"y\"] : self.buffer + piece.pos[\"y\"] + 4,\n",
    "            self.buffer + piece.pos[\"x\"] : self.buffer + piece.pos[\"x\"] + 4,\n",
    "        ] += piece.piecearray\n",
    "        return board\n",
    "\n",
    "    # \"render board\", the array object that will be displayed when\n",
    "    # rendering the game\n",
    "    # has extra information about line clears and such, as well as upscaling\n",
    "    def rboard(self) -> np.ndarray:\n",
    "        t = 2\n",
    "        rboard = self.stamp_piece(self.board, self.curr_piece)\n",
    "        # getting rid of the buffer, but keeping some of it\n",
    "        # around for a border around the board\n",
    "        rboard = rboard[\n",
    "            self.buffer - t : -self.buffer + 1, self.buffer - 1 : -self.buffer + 1\n",
    "        ]\n",
    "\n",
    "        boardmask = np.full_like(rboard, False, dtype=bool)\n",
    "        boardmask[t:-1, 1:-1] = True\n",
    "\n",
    "        # making the pieces on the board distinct from the border\n",
    "        rboard[boardmask] *= 2\n",
    "        rboard[~boardmask] = 1  # and ensuring that the border is\n",
    "        #                         all the same color\n",
    "\n",
    "        # making the next-piece display\n",
    "        next = np.ones((rboard.shape[0], 6))\n",
    "        next[t : t + 4, 1:5] = self.next_piece.piecearray * 2\n",
    "        rboard = np.concatenate([rboard, next], axis=1)\n",
    "        return rboard\n",
    "\n",
    "    # uses cv2 to render the game in a separate window\n",
    "    def render(self, wait: int = 1) -> None:\n",
    "        upscale = 20  # upscale factor\n",
    "        rboard = self.rboard()\n",
    "        rboard = rboard.repeat(upscale, 1).repeat(upscale, 0)\n",
    "        hf = 1.5  # how much space (in terms of one \"tile\") to leave for each\n",
    "        #           line denoting the clear types thus far\n",
    "        rboard = np.concatenate(\n",
    "            [np.ones((4 * int(hf * upscale), rboard.shape[1])), rboard], axis=0\n",
    "        )\n",
    "        rboard /= np.max(rboard)  # for cv2 rgb in [0, 1]\n",
    "\n",
    "        # adding text to upscaled array denoting line clear types\n",
    "        types = [\"singles\", \"doubles\", \"triples\", \"tetrises\"]\n",
    "        for i in range(4):\n",
    "            rboard = cv2.putText(\n",
    "                rboard,\n",
    "                f\"{types[i]}: {self.clear_types[str(i + 1)]}\",\n",
    "                (int(1 * upscale), int((hf * i + 1.5) * upscale)),\n",
    "                0,\n",
    "                fontScale=0.7,\n",
    "                color=255,\n",
    "                thickness=1,\n",
    "            )\n",
    "        rboard = cv2.putText(  # (as well as total lines)\n",
    "            rboard,\n",
    "            f\"total lines: {self.line_clears}\",\n",
    "            (int(1 * upscale), int((hf * 4 + 1.5) * upscale)),\n",
    "            0,\n",
    "            fontScale=0.7,\n",
    "            color=255,\n",
    "            thickness=1,\n",
    "        )\n",
    "        cv2.imshow(\"tetris\", rboard)\n",
    "        cv2.waitKey(wait)  # number of miliseconds per frame of rendering\n",
    "\n",
    "    def new_piece(self) -> None:\n",
    "        self.curr_piece = self.next_piece\n",
    "        self.next_piece = self.piecegen.get_new_piece()\n",
    "        self.n_pieces += 1\n",
    "\n",
    "    # plays one frame of the game\n",
    "    def step(self, act: list) -> bool:\n",
    "        if np.any(\n",
    "            self.stamp_piece(self.board, self.curr_piece)[self.buffer :] > 1\n",
    "        ):\n",
    "            # pieces are intersecting, so a loss\n",
    "            self.reset()\n",
    "\n",
    "        # we will be moving this piece around to see if the desired move\n",
    "        # is valid and if the curr piece should be placed\n",
    "        acted_piece = copy.deepcopy(self.curr_piece)\n",
    "\n",
    "        # doing the actions in the act list;\n",
    "        # in reality, we only do at most 2 actions per frame, but\n",
    "        # this setup means we could do 3 if we wanted\n",
    "        acted_piece.pos[\"x\"] += act[0]\n",
    "        acted_piece.pos[\"y\"] += act[1]\n",
    "        acted_piece.rotate_piece(act[2])\n",
    "\n",
    "        act_board = self.stamp_piece(self.board, acted_piece)\n",
    "        if np.any(act_board[self.buffer :] > 1):\n",
    "            # intersect other piece or wall, move fails\n",
    "            acted_piece = self.curr_piece\n",
    "            # (if move succeeds, implicitly curr_piece <- acted piece)\n",
    "\n",
    "        acted_piece.pos[\"y\"] += 1  # move curr piece down by gravity\n",
    "        down_board = self.stamp_piece(self.board, acted_piece)\n",
    "        if np.any(\n",
    "            down_board[self.buffer :] > 1\n",
    "        ):  \n",
    "            # moving down by gravity fails, so place piece\n",
    "            # note: this does mean that pieces can be moved (\"acted\") on\n",
    "            # the frame when there are no pieces between them and the floor;\n",
    "            # gravity has to act on the piece one more time to make\n",
    "            # it lock into place\n",
    "            acted_piece.pos[\"y\"] -= 1\n",
    "            self.stamp_piece(self.board, acted_piece, copy=False)\n",
    "            self.new_piece()\n",
    "\n",
    "            # remove cleared lines, and record clears\n",
    "            clearmask = (\n",
    "                np.sum(\n",
    "                    self.board[self.buffer : -self.buffer, self.buffer : -self.buffer],\n",
    "                    1,\n",
    "                ) == self.width\n",
    "            )\n",
    "            clears = np.sum(clearmask)\n",
    "\n",
    "            # board without cleared lines\n",
    "            tmpboard = self.board[\n",
    "                self.buffer : -self.buffer, self.buffer : -self.buffer\n",
    "            ][~clearmask]\n",
    "\n",
    "            # move the rest of the pieces down and add empty lines at the top\n",
    "            self.board[self.buffer : -self.buffer, self.buffer : -self.buffer] = (\n",
    "                np.concatenate([np.zeros((clears, self.width)), tmpboard], axis=0)\n",
    "            )\n",
    "\n",
    "            self.line_clears += clears\n",
    "            if clears > 0:\n",
    "                # obv can only get one type of clear per placement\n",
    "                self.clear_types[str(clears)] += 1\n",
    "\n",
    "            return True  # true indicates that piece was placed\n",
    "        else:\n",
    "            self.curr_piece = acted_piece\n",
    "            return False  # false indicates that piece was not placed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we set up the agent and its network. For more details on the network training:\n",
    "\n",
    "DQN: https://arxiv.org/pdf/1312.5602\n",
    "\n",
    "Double DQN: https://arxiv.org/pdf/1509.06461\n",
    "\n",
    "Prioritized Experience Replay: https://arxiv.org/pdf/1511.05952"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper for printing results of training\n",
    "def sixnum(data, r: int = 3) -> list:\n",
    "    ret = [\n",
    "        np.min(data),\n",
    "        np.quantile(data, 0.25),\n",
    "        np.mean(data),\n",
    "        np.median(data),\n",
    "        np.quantile(data, 0.75),\n",
    "        np.max(data),\n",
    "    ]\n",
    "    ret = [float(i.round(r)) for i in ret]\n",
    "    return ret\n",
    "\n",
    "\n",
    "# class for the nn using pytorch\n",
    "class qmodel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(12, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 300),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, 1),\n",
    "        )\n",
    "        self.double()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# agent class, including caching experiences and training\n",
    "class tbot:\n",
    "    def __init__(\n",
    "        self, load_path: str = \"currmodel\", load_cache: bool = True, device: str = \"cpu\"\n",
    "    ):\n",
    "        self.device = device\n",
    "\n",
    "        # instead of storing the objects themselves, we store the state dict of\n",
    "        # the networks and the optimizer, so we first create blank objects...\n",
    "        self.online = qmodel().to(self.device)\n",
    "        self.target = qmodel().to(self.device)\n",
    "        self.target.load_state_dict(self.online.state_dict())\n",
    "        self.optimizer = torch.optim.RAdam(self.online.parameters())\n",
    "        st = 0\n",
    "        cs = int(3e5)\n",
    "\n",
    "        cache = {\n",
    "            \"cache_size\": cs,\n",
    "            \"cached\": np.zeros(cs, dtype=\"object\"),\n",
    "            \"cached_probs\": np.full(cs, -1, dtype=np.float64),\n",
    "            \"n_cached\": 0,\n",
    "        }\n",
    "\n",
    "        if load_path is not None:\n",
    "            # ...and then load the state dict for each one\n",
    "            # (or the obj itself, for the cache)\n",
    "            with open(f\"saves/{load_path}/online.pickle\", \"rb\") as f:\n",
    "                self.online.load_state_dict(pickle.load(f))\n",
    "\n",
    "            with open(f\"saves/{load_path}/target.pickle\", \"rb\") as f:\n",
    "                self.target.load_state_dict(pickle.load(f))\n",
    "\n",
    "            with open(f\"saves/{load_path}/params.pickle\", \"rb\") as f:\n",
    "                params = pickle.load(f)\n",
    "            st = params[\"step\"]\n",
    "\n",
    "            with open(f\"saves/{load_path}/optim.pickle\", \"rb\") as f:\n",
    "                self.optimizer.load_state_dict(pickle.load(f))\n",
    "\n",
    "            if load_cache:\n",
    "                with open(f\"saves/{load_path}/cache.pickle\", \"rb\") as f:\n",
    "                    cache = pickle.load(f)\n",
    "\n",
    "        for p in self.target.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.optimizer.param_groups[0][\"lr\"] = 2.5e-4\n",
    "\n",
    "        #RAdamW optim\n",
    "        self.optimizer.param_groups[0][\"decoupled_weight_decay\"] = True\n",
    "\n",
    "        self.cache_size = cache[\"cache_size\"]\n",
    "        self.cached = cache[\"cached\"]\n",
    "        self.cached_probs = cache[\"cached_probs\"]\n",
    "        self.n_cached = cache[\"n_cached\"]\n",
    "\n",
    "        # lots of hyperparams below :)\n",
    "        # steps to wait for cache burn in\n",
    "        self.wait_to_train = 1e4\n",
    "\n",
    "        # total times the agent has made a decision about a move and\n",
    "        # cached the result\n",
    "        self.step = st\n",
    "\n",
    "        # epsilon controls exploration vs exploitation, starts high and decays\n",
    "        # linearly\n",
    "        self.eps_min = 0.1\n",
    "        self.eps_max = 1\n",
    "        self.eps_steps = 1e6 + self.wait_to_train\n",
    "        self.epsilon = max(\n",
    "            (\n",
    "                self.eps_max\n",
    "                - (self.eps_max - self.eps_min) * (self.step / self.eps_steps)\n",
    "            ),\n",
    "            self.eps_min,\n",
    "        )\n",
    "\n",
    "        # beta controls the bias-annealing for prioritized experience replay\n",
    "        self.beta_max = 1\n",
    "        self.beta_min = 0.4\n",
    "        self.beta_steps = 5e6 + self.wait_to_train\n",
    "        self.correction_beta = min(\n",
    "            (\n",
    "                self.beta_min\n",
    "                + (self.beta_max - self.beta_min) * (self.step / self.beta_steps)\n",
    "            ),\n",
    "            self.beta_max,\n",
    "        )\n",
    "\n",
    "        # other hyperparams for prioritized experience replay\n",
    "        self.delta_epsilon = 1e-3\n",
    "        self.stoch_alpha = 0.6\n",
    "        self.do_prioritization = True\n",
    "\n",
    "        # this is used for recording which cache entries\n",
    "        # have not been assigned a priority measure\n",
    "        self.not_measured_inds = []\n",
    "\n",
    "        # how often (in steps) and how large of a batch to sample\n",
    "        # from the cache\n",
    "        self.train_every = 4\n",
    "        self.batch_size = 32\n",
    "\n",
    "        # steps between saves to currmodel or to create a new checkpoint folder\n",
    "        self.checkpoint_every = 1e5\n",
    "        self.save_every = 1e4\n",
    "\n",
    "        # rate of syncing and discount rate for double dqn\n",
    "        self.sync_every = 1e4\n",
    "        self.discount_rate = 1 - 5e-4\n",
    "\n",
    "        # just used for recording results during training to get an idea of\n",
    "        # what's going on\n",
    "        self.prev_loss = []\n",
    "        self.prev_est = []\n",
    "        self.prev_target = []\n",
    "        self.prev_rew = []\n",
    "\n",
    "        # loss fun for optimization\n",
    "        self.loss_fn = nn.L1Loss(reduction=\"sum\")\n",
    "\n",
    "    # eps and beta will both decay linearly (though in opposite directions)\n",
    "    def update_eps(self) -> None:\n",
    "        if self.n_cached > self.wait_to_train:\n",
    "            self.epsilon = max(\n",
    "                (\n",
    "                    self.eps_max\n",
    "                    - (self.eps_max - self.eps_min) * (self.step / self.eps_steps)\n",
    "                ),\n",
    "                self.eps_min,\n",
    "            )\n",
    "            self.correction_beta = min(\n",
    "                (\n",
    "                    self.beta_min\n",
    "                    + (self.beta_max - self.beta_min) * (self.step / self.beta_steps)\n",
    "                ),\n",
    "                self.beta_max,\n",
    "            )\n",
    "\n",
    "    # given features for various states, return the index of the state with the\n",
    "    # greatest estimated q value\n",
    "    # (or act randomly, contingent on epsilon)\n",
    "    # force_model just only uses the network, and ensures that the agent\n",
    "    # does not update its parameters as if it is training; essentially\n",
    "    # \"freezes\" the agent's training progress\n",
    "    def act(self, Xps: torch.Tensor, force_model: bool = False) -> int:\n",
    "        if force_model or (np.random.rand() > self.epsilon):\n",
    "            with torch.no_grad():\n",
    "                self.online.eval()\n",
    "                a_id = int(torch.argmax(self.online(Xps).flatten()).item())\n",
    "        else:\n",
    "            a_id = int(np.random.rand() * (Xps.shape[0]))\n",
    "\n",
    "        if not force_model:\n",
    "            self.update_eps()\n",
    "            self.step += 1\n",
    "\n",
    "        return a_id\n",
    "\n",
    "    # an experience tuple consists of\n",
    "    # (state_1 features, possible_next_states features,\n",
    "    # reward for state_1, game_end (binary))\n",
    "    # new entries are added to empty spots until full, and then the oldest\n",
    "    # entries are replaced first (like a circular array)\n",
    "    def cache(self, experience: tuple[torch.Tensor, torch.Tensor, int, int]) -> None:\n",
    "        newind = (self.n_cached) % self.cache_size\n",
    "        self.cached[newind] = experience\n",
    "        self.cached_probs[newind] = self.delta_epsilon\n",
    "        if self.do_prioritization:\n",
    "            self.not_measured_inds.append(newind)\n",
    "\n",
    "        self.n_cached += 1\n",
    "\n",
    "    # for prioritization\n",
    "    def compute_cache_probs(self) -> np.ndarray:\n",
    "        n = min(self.cache_size, self.n_cached)\n",
    "        stoch_probs = np.power(self.cached_probs[:n], self.stoch_alpha)\n",
    "        return stoch_probs / np.sum(stoch_probs)\n",
    "\n",
    "    # randomly sample cache entries (according to cache sampling scheme),\n",
    "    # return indices\n",
    "    def recall(self) -> np.ndarray:\n",
    "        p = np.cumsum(self.compute_cache_probs())\n",
    "        inds = np.searchsorted(p, np.random.rand(self.batch_size))\n",
    "        return inds\n",
    "\n",
    "    # save curr model as a checkpoint\n",
    "    def save_checkpoint(self) -> None:\n",
    "        d = datetime.datetime.today()\n",
    "        path = (\n",
    "            \"checkpoints/\"\n",
    "            + f\"{d.year}-{d.month}-{d.day}-{d.hour}-{d.minute}-{d.second}\"\n",
    "        )\n",
    "        self.save_models(path=path)\n",
    "\n",
    "    # save a snapshot of the current model status\n",
    "    # (including networks, optim, steps, and cache) to path in saves folder\n",
    "    # note that this requires that a \"saves\" folder exists in the same\n",
    "    # dir as this file\n",
    "    # save cache is optional because larger caches can take >1 minute to save\n",
    "    # (...on my machine that was saving to a SSD, so it\n",
    "    # may be excruciatingly long for a HDD)\n",
    "    def save_models(self, path: str = \"currmodel\", save_cache: bool = True) -> None:\n",
    "        if \"saves\" not in os.listdir():\n",
    "            os.mkdir(\"saves\")\n",
    "\n",
    "        path_parts = path.split(\"/\")\n",
    "        curr = \"saves\"\n",
    "        for p in path_parts:\n",
    "            if p not in os.listdir(curr):\n",
    "                os.mkdir(f\"{curr}/{p}\")\n",
    "            curr += \"/\" + p\n",
    "\n",
    "        for p in [\n",
    "            \"online.pickle\",\n",
    "            \"target.pickle\",\n",
    "            \"optim.pickle\",\n",
    "            \"params.pickle\",\n",
    "            \"cache.pickle\",\n",
    "        ]:\n",
    "            if p not in os.listdir(f\"saves/{path}\"):\n",
    "                open(f\"saves/{path}/{p}\", \"x\").close()\n",
    "\n",
    "        with open(f\"./saves/{path}/online.pickle\", \"wb\") as f:\n",
    "            pickle.dump(self.online.state_dict(), f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        with open(f\"./saves/{path}/target.pickle\", \"wb\") as f:\n",
    "            pickle.dump(self.target.state_dict(), f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        with open(f\"./saves/{path}/optim.pickle\", \"wb\") as f:\n",
    "            pickle.dump(self.optimizer.state_dict(), f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        with open(f\"./saves/{path}/params.pickle\", \"wb\") as f:\n",
    "            pickle.dump({\"step\": self.step}, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        if save_cache:\n",
    "            with open(f\"./saves/{path}/cache.pickle\", \"wb\") as f:\n",
    "                pickle.dump(\n",
    "                    {\n",
    "                        \"n_cached\": self.n_cached,\n",
    "                        \"cached\": self.cached,\n",
    "                        \"cached_probs\": self.cached_probs,\n",
    "                        \"cache_size\": self.cache_size,\n",
    "                    },\n",
    "                    f,\n",
    "                    pickle.HIGHEST_PROTOCOL,\n",
    "                )\n",
    "\n",
    "    # batch together entries of the cache for use in training\n",
    "    def cachetodict(\n",
    "        self, cache_entries: torch.Tensor\n",
    "    ) -> dict[torch.Tensor, np.ndarray, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        return {\n",
    "            \"s\": torch.concat([c[0].to(self.device) for c in cache_entries], dim=0),\n",
    "\n",
    "            # this gives us indices where the s' for one state ends and\n",
    "            # another begins, since they are all batched together\n",
    "            \"divs\": np.cumsum([0] + [c[1].shape[0] for c in cache_entries]),\n",
    "            \n",
    "            \"s'\": torch.concat([c[1].to(self.device) for c in cache_entries], dim=0),\n",
    "            \"r\": torch.tensor([c[2] for c in cache_entries], device=self.device),\n",
    "            \"d\": torch.tensor([c[3] for c in cache_entries], device=self.device),\n",
    "        }\n",
    "\n",
    "    # do a backpropogation step\n",
    "    def update_online(self) -> None:\n",
    "        if self.do_prioritization:\n",
    "            self.get_new_measures()\n",
    "\n",
    "        recall_inds = self.recall()\n",
    "        r = self.cachetodict(self.cached[recall_inds])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.online.eval()\n",
    "            self.target.eval()\n",
    "            lines = self.online(r[\"s'\"]).flatten()\n",
    "\n",
    "            # inds for the s' with max q value as\n",
    "            # determined by online\n",
    "            inds = torch.tensor(  \n",
    "                [\n",
    "                    torch.argmax(lines[r[\"divs\"][j] : r[\"divs\"][j + 1]]).item()\n",
    "                    + r[\"divs\"][j]\n",
    "                    for j in range(r[\"divs\"].shape[0] - 1)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            ap = r[\"s'\"][inds]\n",
    "            Qpt = self.target(ap).flatten()  # target q-values for those s' from above\n",
    "\n",
    "            # bellman equation :)\n",
    "            # use 1-r[\"d\"] to ensure that a terminal state's reward is\n",
    "            # calculated correctly;\n",
    "            # if s is a terminal state, its cumulative reward is\n",
    "            # just its reward alone\n",
    "            td_target =  r[\"r\"] + (1 - r[\"d\"]) * self.discount_rate * Qpt\n",
    "\n",
    "        self.online.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        td_est = self.online(r[\"s\"]).flatten()\n",
    "        delta_torch = torch.abs((td_target - td_est).detach())\n",
    "        delta = delta_torch.cpu().numpy()\n",
    "\n",
    "        if self.do_prioritization:\n",
    "            # bias anealling, see paper for details\n",
    "            N = min(self.cache_size, self.n_cached)\n",
    "            computed_cached_probs = self.compute_cache_probs()\n",
    "            weight_corrections = np.power(\n",
    "                computed_cached_probs[recall_inds] * N, -self.correction_beta\n",
    "            )\n",
    "            weight_corrections = weight_corrections / np.max(weight_corrections)\n",
    "            weight_corrections = torch.from_numpy(weight_corrections).to(self.device)\n",
    "\n",
    "            # (we can do this multiplication on the values\n",
    "            # themselves instead of the losses\n",
    "            # since we use L1 loss; they are mathematically equivalent\n",
    "            # since the weights are > 0)\n",
    "            td_est_corr = td_est * weight_corrections\n",
    "            td_target_corr = td_target * weight_corrections\n",
    "\n",
    "            loss = self.loss_fn(td_est_corr, td_target_corr)\n",
    "            \n",
    "            self.cached_probs[recall_inds] =  delta + self.delta_epsilon\n",
    "        else:\n",
    "            loss = self.loss_fn(td_est, td_target)\n",
    "\n",
    "        # recording some results from this training step for information\n",
    "        self.prev_loss.append(np.mean(delta))\n",
    "        self.prev_est.append(td_est.mean().item())\n",
    "        self.prev_target.append(td_target.mean().item())\n",
    "        self.prev_rew.append(r[\"r\"].double().mean().item())\n",
    "\n",
    "        # print summary of training results for information\n",
    "        if self.step % (40 * self.train_every) == 0:\n",
    "            print(\"est:\", sixnum(self.prev_est))\n",
    "            print(\"target:\", sixnum(self.prev_target))\n",
    "            print(\"loss:\", sixnum(self.prev_loss))\n",
    "            print(\"rew:\", sixnum(self.prev_rew))\n",
    "            self.prev_loss = []\n",
    "            self.prev_est = []\n",
    "            self.prev_target = []\n",
    "            self.prev_rew = []\n",
    "\n",
    "        # update weights, finally\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    # for entries that do not have prioritization measures, get the measures by\n",
    "    # getting the td error\n",
    "    # (basically a truncated version of update_online)\n",
    "    def get_new_measures(self) -> None:\n",
    "        notmeasured = np.array(self.not_measured_inds)\n",
    "        r = self.cachetodict(self.cached[notmeasured])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.online.eval()\n",
    "            self.target.eval()\n",
    "            lines = self.online(r[\"s'\"]).flatten()\n",
    "            inds = torch.tensor(\n",
    "                [\n",
    "                    torch.argmax(lines[r[\"divs\"][j] : r[\"divs\"][j + 1]]).item()\n",
    "                    + r[\"divs\"][j]\n",
    "                    for j in range(r[\"divs\"].shape[0] - 1)\n",
    "                ]\n",
    "            )\n",
    "            ap = r[\"s'\"][inds]\n",
    "            Qpt = self.target(ap).flatten()\n",
    "            td_target = r[\"r\"] + (1 - r[\"d\"]) * self.discount_rate * Qpt\n",
    "            td_est = self.online(r[\"s\"]).flatten()\n",
    "\n",
    "        delta = np.abs((td_target - td_est).cpu().detach().numpy())\n",
    "        self.cached_probs[notmeasured] = delta + self.delta_epsilon\n",
    "\n",
    "        self.not_measured_inds = []\n",
    "\n",
    "    # this will be run at every step to determine what to do with the model\n",
    "    def learn(self) -> None:\n",
    "        if self.n_cached > self.wait_to_train:\n",
    "            if self.step % self.train_every == 0:\n",
    "                self.update_online()\n",
    "\n",
    "            if (self.step % self.sync_every == 0): \n",
    "                self.target.load_state_dict(self.online.state_dict())\n",
    "                for p in self.target.parameters():\n",
    "                    p.requires_grad = False\n",
    "\n",
    "                # since target model has updated, throw away previous training\n",
    "                # values, since they dont represent the current model\n",
    "                self.prev_loss = []\n",
    "                self.prev_est = []\n",
    "                self.prev_target = []\n",
    "                self.prev_rew = []\n",
    "\n",
    "        if self.step % self.checkpoint_every == 0:\n",
    "            self.save_checkpoint()\n",
    "\n",
    "        if self.step % self.save_every == 0:\n",
    "            # we can save a lot of time by copying over the most recent\n",
    "            # checkpoint cache, since saving the current cache every time would\n",
    "            # be too slow\n",
    "            try:\n",
    "                most_recent = sorted(os.listdir(\"saves/checkpoints\"))[-1]\n",
    "            except FileNotFoundError or IndexError:\n",
    "                self.save_models(save_cache=True)\n",
    "            else:\n",
    "                self.save_models(save_cache=False)\n",
    "                recent_cache = f\"saves/checkpoints/{most_recent}/cache.pickle\"\n",
    "                curr_cache = \"saves/currmodel/cache.pickle\"\n",
    "                shutil.copyfile(recent_cache, curr_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we set up some various helper functions for the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a rotation and movement to do, creates a list of actions that\n",
    "# can be performed one-by-one in the game to achieve it\n",
    "# this also makes sure to make the list of actions as short as possible\n",
    "# an action is a 3-int list where\n",
    "# [x-movement, y-movement, rotation]\n",
    "# where +1 rotates right and -1 rotates left\n",
    "# and similarly for the movements\n",
    "def construct_actionlist(rotation: int, move: int) -> list:\n",
    "    rot = np.sign(rotation)\n",
    "    r = abs(rotation)\n",
    "    mov = np.sign(move)\n",
    "    m = abs(move)\n",
    "\n",
    "    ret = np.zeros((max(r, m), 3), dtype=int)\n",
    "    ret[:m, 0] = int(mov)\n",
    "    ret[:r, 2] = int(rot)\n",
    "    return ret.tolist()\n",
    "\n",
    "\n",
    "# given starting states and a block to place, returns all of the possible\n",
    "# next states from each of the starting states\n",
    "# as well as their corresponding actions\n",
    "# this seemingly simple function is so monstrous because it's vectorized;\n",
    "# if it was made shorter using python loops it would run too slow\n",
    "def get_sps(\n",
    "    states: np.ndarray, block: piece\n",
    ") -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    safety = 0\n",
    "\n",
    "    # make sure piece is in default position\n",
    "    block.rotate_piece(-block.orient)\n",
    "\n",
    "    heights = np.sum(np.cumsum(states, axis=1) > 0, axis=1)\n",
    "\n",
    "    buffer = 3\n",
    "\n",
    "    # starting states still batched, but with buffer\n",
    "    c = np.ones(\n",
    "        (states.shape[0], states.shape[1] + 2 * buffer, states.shape[2] + 2 * buffer)\n",
    "    )\n",
    "    c[:, buffer:-buffer, buffer:-buffer] = states\n",
    "\n",
    "    # from very left==0 the \"middle\" of the board is at index 3\n",
    "    # (when placing piece via binary array)\n",
    "    middle = 3\n",
    "\n",
    "    # given that the array is 4x4, there are (boardwidth+2*buffer)-4 places\n",
    "    # to put it on the board without indexing out of bounds\n",
    "    movements = np.arange(c.shape[2] - 4)\n",
    "    rotations = np.array(block.rotations)\n",
    "    mesh = np.meshgrid(movements, rotations)\n",
    "\n",
    "    # all combinations of movement + rotation in a nx2 array\n",
    "    mr = np.concatenate(\n",
    "        [mesh[0].flatten()[:, None], mesh[1].flatten()[:, None]], axis=1\n",
    "    )\n",
    "    longmr = np.tile(mr, (c.shape[0], 1))\n",
    "\n",
    "    # now, there is a starting state for each movement+rotation\n",
    "    # combination for each starting state\n",
    "    longc = c.repeat(mr.shape[0], axis=0)\n",
    "\n",
    "    # retain original indices of each starting state in the original batch\n",
    "    c_inds = np.arange(c.shape[0]).repeat(mr.shape[0])\n",
    "\n",
    "    # now, have the associated rotated array of the block alongside\n",
    "    # each starting state + movement + rotation combination\n",
    "    longblocks = np.zeros((longmr.shape[0], 4, 4))\n",
    "    for i in block.rotations:\n",
    "        block.rotate_piece(i - block.orient)\n",
    "        longblocks[longmr[:, 1] == i] = block.piecearray[None]\n",
    "    block.rotate_piece(-block.orient)\n",
    "\n",
    "    # now, start working on minimum falling distance for each tetromino\n",
    "    mid_move = longmr.copy()\n",
    "    mid_move[:, 0] -= buffer + middle\n",
    "\n",
    "    # if going to be doing no movement and no rotation, then\n",
    "    # min-falling is just 0, so leave it alone\n",
    "    mask = np.sum(np.abs(mid_move), axis=1) != 0\n",
    "    minl = np.zeros(longc.shape[0])\n",
    "\n",
    "    # if you are going to be doing some movement, then min fall is equal to\n",
    "    # the number of actions you have to do on the way down\n",
    "    # (since gravity acts once per frame and a movement\n",
    "    # and rotation can be done together in one frame)\n",
    "    # (plus a safety factor)\n",
    "    minl[mask] = np.max(np.abs(mid_move[mask]), axis=1) + safety\n",
    "\n",
    "    p = 2  # pieces start 2 tiles above the ceiling\n",
    "\n",
    "    # this gives us the starting height of each tile of the 4x4 tetromino array\n",
    "    ha = (\n",
    "        longblocks * np.arange(\n",
    "            states.shape[1] + p, \n",
    "            states.shape[1] + p - 4, \n",
    "            step=-1\n",
    "            )[None, :, None]\n",
    "    )\n",
    "\n",
    "    # this is the minimum falling dist for each tetromino to make sure that\n",
    "    # none of the tetromino is in the ceiling, which would be bad\n",
    "    ha_max = np.max(ha, axis=(1, 2)) - states.shape[1]\n",
    "    minl[ha_max > minl] = ha_max[ha_max > minl]\n",
    "\n",
    "    h_large = 100\n",
    "    ha[ha == 0] = h_large\n",
    "\n",
    "    # lowest starting height for each column of the tetromino\n",
    "    ha = np.min(ha, axis=1)\n",
    "\n",
    "    # include the buffer in the heights to make sure that it doesnt think\n",
    "    # it can place blocks in the wall; make the height of the buffer as\n",
    "    # high as the wall\n",
    "    heights = np.concatenate(\n",
    "        [\n",
    "            np.full((states.shape[0], buffer), states.shape[1]),\n",
    "            heights,\n",
    "            np.full((states.shape[0], buffer), states.shape[1]),\n",
    "        ],\n",
    "        axis=1,\n",
    "    ) \n",
    "\n",
    "    # make sure heights line up with repeated starting states\n",
    "    heightsr = heights.repeat(mr.shape[0], axis=0)\n",
    "\n",
    "    # for each movement option, we have to consider each of the 4\n",
    "    # columns of the tetromino array\n",
    "    indx = longmr[:, 0:1] + np.arange(4)[None]\n",
    "\n",
    "    # take the distance between the bottom of each column of the tetromino\n",
    "    # and the blocks below (after it has been moved and rotated),\n",
    "    # and take the min of them;\n",
    "    # that is the amount of tiles that the tetromino can fall before being\n",
    "    # placed, and so it's how much time it has to act and move it and such\n",
    "    abletofall = (\n",
    "        np.min(ha - heightsr[np.arange(heightsr.shape[0])[:, None], indx], axis=1) - 1\n",
    "    )\n",
    "\n",
    "    # if the amount it can fall is not enough, then that\n",
    "    # movement + rotation is invalid\n",
    "    mask = abletofall < minl\n",
    "\n",
    "    p_rot = np.arange(4) - 1\n",
    "    inds = np.arange(c.shape[0])\n",
    "\n",
    "    # the coord cube:\n",
    "    # a \"cube\" boolean mask where each dimension corresponds to a rotation,\n",
    "    # movement, or starting-state index\n",
    "    # there are a few extra restrictions on movement that we solve with the\n",
    "    # coord cube:\n",
    "    coord_cube = np.ones(\n",
    "        (p_rot.shape[0], inds.shape[0], movements.shape[0]), dtype=bool\n",
    "    )\n",
    "\n",
    "    # -anything where abletofall < min_fall is obv invalid\n",
    "    coord_cube[longmr[:, 1][mask] + 1, c_inds[mask], longmr[:, 0][mask]] = False\n",
    "\n",
    "    # -if a given movement+rotation+starting state is invalid, then\n",
    "    #  any movement going further in the same direction should be discarded\n",
    "    coord_cube[:, :, buffer + middle :] = np.cumprod(\n",
    "        coord_cube[:, :, buffer + middle :], axis=2\n",
    "    )\n",
    "    coord_cube[:, :, : buffer + middle + 1] = np.cumprod(\n",
    "        coord_cube[:, :, : buffer + middle + 1][:, :, ::-1], axis=2\n",
    "    )[:, :, ::-1]\n",
    "\n",
    "    # -if the no-action is invalid for a given starting state, then\n",
    "    #  make everything for that starting state invalid\n",
    "    coord_cube = (\n",
    "        coord_cube * coord_cube[1, :, buffer + middle][None, :, None]\n",
    "    )\n",
    "\n",
    "    # now, match up coord cube mask to\n",
    "    # starting states + movement + rotation combinations\n",
    "    retmask = coord_cube[longmr[:, 1] + 1, c_inds, longmr[:, 0]]\n",
    "\n",
    "    # return values\n",
    "    retc = longc[retmask]\n",
    "    retblocks = longblocks[retmask]\n",
    "    retmr = longmr[retmask]\n",
    "    retfall = abletofall[retmask] - p + buffer\n",
    "    retcind = c_inds[retmask]\n",
    "\n",
    "    # however, we now need to actually place the tetrominoes on the\n",
    "    # starting states that are valid\n",
    "    # the blocks are already rotated, so just need to move and fall\n",
    "    mr_rep = retmr[:, 0:1] + np.arange(4)[None]\n",
    "    fall_rep = retfall[:, None] + np.arange(4)[None]\n",
    "    mr_matr = mr_rep[:, None].repeat(4, axis=1)\n",
    "    fall_matr = fall_rep[:, None].repeat(4, axis=1).transpose((0, 2, 1))\n",
    "    \"\"\"\n",
    "    above, essentially doing:\n",
    "    1 1 1 1     1 2 3 4\n",
    "    2 2 2 2 and 1 2 3 4\n",
    "    3 3 3 3     1 2 3 4\n",
    "    4 4 4 4     1 2 3 4 \n",
    "    for the coordinate offsets to get placement coordinates for each tile\n",
    "    (notice how theyre transposes)\n",
    "    \"\"\"\n",
    "\n",
    "    retc[\n",
    "        # 16 tiles to place for each state\n",
    "        np.arange(retc.shape[0]).repeat(4 * 4),\n",
    "        fall_matr.flatten().astype(int),\n",
    "        mr_matr.flatten().astype(int),\n",
    "    ] += retblocks.flatten()\n",
    "\n",
    "    retmr[:, 0] -= buffer + middle  # since the very left was 0 before\n",
    "\n",
    "    retc = retc[:, buffer:-buffer, buffer:-buffer]\n",
    "\n",
    "    # we return the after-states, the movement+rotations to get to those\n",
    "    # after-states, and the indices of each after-state corresponding to\n",
    "    # their starting-state in the \"states\" argument\n",
    "    return (retc, retmr, retcind,)\n",
    "\n",
    "# get features for a batch of tetris board states\n",
    "# features are\n",
    "# heights of each column, number of line clears, and number of holes\n",
    "def batchedfeatures(\n",
    "    states: np.ndarray,\n",
    "    device: str = \"cpu\",\n",
    "    clip_features: bool = True,\n",
    "    exlines: np.ndarray = None,\n",
    "    exlines_operation: str = \"max\",\n",
    ") -> torch.Tensor:\n",
    "    states = torch.from_numpy(states).to(device)\n",
    "\n",
    "    clearmask = torch.sum(states, dim=2) == 10\n",
    "    clears = torch.sum(clearmask, dim=1, keepdim=True, dtype=torch.double)\n",
    "\n",
    "    clearsums = torch.cumsum(clearmask, dim=1)[:, :, None]\n",
    "    statesums = torch.cumsum(states, dim=1)\n",
    "    colholes = torch.sum(((statesums - clearsums) > 0) & (states == 0), dim=1)\n",
    "    holes = torch.sum(colholes, dim=1, keepdim=True, dtype=torch.double)\n",
    "\n",
    "    numblocks = torch.sum(states, dim=1) - clears\n",
    "    heights = (numblocks + colholes).double()\n",
    "\n",
    "    # when we do lookahead, the features for a state do not include the lines\n",
    "    # achieved after the first block was placed, so we pass those clears in\n",
    "    # manually\n",
    "    if exlines is not None:\n",
    "        exlines = torch.from_numpy(exlines).to(device).double()\n",
    "        clears = clears[:, 0]\n",
    "\n",
    "        match exlines_operation:\n",
    "            case \"sum\":\n",
    "                clears += exlines\n",
    "            case _:\n",
    "                clears[clears < exlines] = exlines[clears < exlines]\n",
    "\n",
    "        clears = clears[:, None]\n",
    "\n",
    "    if clip_features:\n",
    "        maxholes = 60\n",
    "        maxclears = 4\n",
    "        maxheight = 20\n",
    "\n",
    "        heights = heights / maxheight\n",
    "        clears = clears / maxclears\n",
    "        holes = holes / maxholes\n",
    "\n",
    "    return torch.cat([heights, holes, clears], dim=1)\n",
    "\n",
    "\n",
    "# given some states, remove the line clears and return the cleared states\n",
    "# and the number of lines from each one\n",
    "def remove_lines(states: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "    mask1 = np.sum(states, axis=2) != states.shape[2]\n",
    "    lines = np.sum(~mask1, axis=1)\n",
    "    maxlines = np.max(lines)\n",
    "    t = np.concatenate(\n",
    "        [np.zeros((states.shape[0], maxlines, states.shape[2])), states], axis=1\n",
    "    )\n",
    "    mask2 = np.concatenate(\n",
    "        [np.ones((mask1.shape[0], maxlines), dtype=bool), mask1], axis=1\n",
    "    )\n",
    "    mask2 *= np.cumsum(mask2[:, ::-1], axis=1)[:, ::-1] <= states.shape[1]\n",
    "\n",
    "    return (\n",
    "        t.reshape(t.shape[1] * t.shape[0], t.shape[2])[mask2.flatten()].reshape(\n",
    "            states.shape\n",
    "        ),\n",
    "        lines,\n",
    "    )\n",
    "\n",
    "\n",
    "# wrapper for get_sps that mirrors the structure of get_sps_twoblock\n",
    "def get_sps_one_block(\n",
    "    state: np.ndarray, block: piece\n",
    ") -> dict[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    first = get_sps(state[None], block)\n",
    "    return {\n",
    "        \"sps\": first[0],\n",
    "        \"movement_rotation\": first[1],\n",
    "        \"old_state_inds\": first[2],\n",
    "        \"exlines\": np.zeros((first[0].shape[0],)),\n",
    "    }\n",
    "\n",
    "\n",
    "# given two blocks, get all of the possible states after placing both blocks\n",
    "# by calling get_sps twice\n",
    "# this is how we do lookahead; consider all ways to place both blocks,\n",
    "# compute features for all after-states, and then choose the best\n",
    "# ensures that the states are accurate by removing lines that are cleared\n",
    "# after placing the first block\n",
    "# also returns the line clears that were achieved after placing the first block\n",
    "def get_sps_two_blocks(\n",
    "    state: np.ndarray, block1: piece, block2: piece\n",
    ") -> dict[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    first = get_sps(state[None], block1)\n",
    "    rmlines = remove_lines(first[0])\n",
    "    second = get_sps(rmlines[0], block2)\n",
    "    moves = first[1][second[2]]\n",
    "    exlines = rmlines[1][second[2]]\n",
    "    return {\n",
    "        \"sps\": second[0],\n",
    "        \"movement_rotation\": moves,\n",
    "        \"old_state_inds\": second[2],\n",
    "        \"exlines\": exlines,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for the actual training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: i have only tested loading and saving networks on a windows machine,\n",
    "# i dont see why it shouldnt work elsewhere, but there is no guarentee\n",
    "\n",
    "# TO TRAIN YOUR OWN NETWORK:\n",
    "# -either set usecurr to false or set load_path to None, and run this cell\n",
    "# -when the bot eventually saves, in this current directory, if it doesnt exist,\n",
    "#  a folder called \"saves\" will be created, and another folder called \"currmodel\"\n",
    "#  will be created inside of it. Additionally,\n",
    "#  a folder called \"checkpoints\" will periodically save\n",
    "#  timestamped model checkpoints\n",
    "# -the saved bot can be run by setting load_path to \"currmodel\", or\n",
    "#  run a checkpoint by setting load_path to \"checkpoints/{checkpoint name}\"\n",
    "\n",
    "# For training:\n",
    "# set force_model to False (so that it actually trains)\n",
    "# for a large training speedup, set render and midrender to False\n",
    "\n",
    "\n",
    "usecurr = True  # use a saved model or start fresh\n",
    "force_model = True  # force network eval or do training\n",
    "use_queue = True  # use lookahead by considering next block\n",
    "render = True  # render the game as it's played\n",
    "mid_render = True  # render the blocks falling, or skip the falling animation\n",
    "fps = 1e3  # playing speed in frames per second, maximum 1e3\n",
    "# (fps may be innacurate at higher values due to computation speed)\n",
    "\n",
    "# examples of \"good\" models from training\n",
    "# 2mil is most safe, 6mil is most reckless (goes for full tetrises),\n",
    "# and 3mil is in-between\n",
    "paths = [\"steps_2mil\", \"steps_3mil\", \"steps_6mil\"]\n",
    "\n",
    "# *****change this value here to use one of the other pre-trained models*****\n",
    "load_path = paths[2]\n",
    "\n",
    "bot = tbot(load_path=load_path if usecurr else None, load_cache=not force_model)\n",
    "env = tetris_board()\n",
    "\n",
    "render_wait = max(int(1e3 / fps), 1)\n",
    "\n",
    "# note: the game will play and the training/testing loop will\n",
    "# run until the cell is interrupted\n",
    "while True:\n",
    "    done = False\n",
    "\n",
    "    print(\"======================================================\")\n",
    "    print(f\"steps: {bot.step}\")\n",
    "    print(f\"eps: {bot.epsilon}\")\n",
    "\n",
    "    env.reset()\n",
    "\n",
    "    prev_lines = env.line_clears\n",
    "\n",
    "    st = 1\n",
    "\n",
    "    while True:\n",
    "        # line clears for this state; clears that happened between\n",
    "        # last state and this one\n",
    "        curr_lines = env.line_clears - prev_lines\n",
    "\n",
    "        prev_lines = env.line_clears\n",
    "\n",
    "        # reward function; 1 for each piece placed plus a quadratic\n",
    "        # factor of the line clears (similar to NES tetris scoring)\n",
    "        state_reward = 1 + curr_lines**2\n",
    "\n",
    "        curr_state = env.get_wboard()\n",
    "\n",
    "        if force_model and use_queue:\n",
    "            sps = get_sps_two_blocks(curr_state, env.curr_piece, env.next_piece)\n",
    "        else:\n",
    "            sps = get_sps_one_block(curr_state, env.curr_piece)\n",
    "\n",
    "        # this indicates that there is nowhere to place the\n",
    "        # current tetromino (or two tetrominoes), so a loss\n",
    "        if sps[\"sps\"].shape[0] == 0:  \n",
    "            done = True\n",
    "\n",
    "            # this doesnt really matter since the model value for this\n",
    "            # will never be used\n",
    "            sps_features = torch.zeros(\n",
    "                (1, 12), device=bot.device, dtype=torch.float64\n",
    "            )\n",
    "        else:\n",
    "            # features of the upcoming states;\n",
    "            # read sps as \"s-primes\", multiple of s'\n",
    "            sps_features = batchedfeatures(\n",
    "                sps[\"sps\"], device=bot.device, exlines=sps[\"exlines\"]\n",
    "            )\n",
    "\n",
    "            # index of the state in sps with the greatest estimated q-value;\n",
    "            # we will use this index to access everything else\n",
    "            # about this state from sps\n",
    "            bot_act_ind: int = bot.act(sps_features, force_model=force_model)\n",
    "            move_rot = sps[\"movement_rotation\"]\n",
    "            actions: list = construct_actionlist(\n",
    "                move_rot[bot_act_ind, 1], move_rot[bot_act_ind, 0]\n",
    "            )\n",
    "            push_down = np.zeros((30, 3), dtype=int)\n",
    "            push_down[:, 1] = 1\n",
    "            actions += push_down.tolist()\n",
    "\n",
    "            placed = False\n",
    "\n",
    "            # perform each actual action and then move down until placed\n",
    "            for m in actions:\n",
    "                placed = env.step(m)\n",
    "\n",
    "                if placed:\n",
    "                    break\n",
    "\n",
    "                if render and mid_render:  # rendering block on its way down\n",
    "                    env.render(render_wait)\n",
    "\n",
    "            if not force_model:  # force model is esentially eval mode\n",
    "                bot.learn()\n",
    "\n",
    "        # at st==1, the curr state was not \"reached\" by any means,\n",
    "        # so the bot doesnt have to know how to estimate its q-value\n",
    "        if st != 1 and not force_model:\n",
    "            curr_features = batchedfeatures(\n",
    "                curr_state[None], device=bot.device, exlines=np.array(curr_lines)[None]\n",
    "            )\n",
    "            bot.cache((curr_features, sps_features, state_reward, int(done)))\n",
    "\n",
    "        st += 1\n",
    "        if render:\n",
    "            env.render(render_wait)\n",
    "\n",
    "        if done:\n",
    "            print(\n",
    "                \"total reward:\",\n",
    "                env.n_pieces\n",
    "                + np.sum(\n",
    "                    [env.clear_types[str(i + 1)] * (i + 1) ** 2 for i in range(4)]\n",
    "                ),\n",
    "            )\n",
    "            print(\"total lines:\", env.line_clears)\n",
    "            print(\"total pieces:\", env.n_pieces)\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tetris3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
